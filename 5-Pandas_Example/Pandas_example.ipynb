{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top\" id=\"top\"></a>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <h1>CHE597 - Pandas Example</h1>\n",
    "    <a href=\"https://github.com/bernalde\">David E. Bernal Neira</a>\n",
    "    <br>\n",
    "    <i>Davidson School of Chemical Engineering, Purdue University</i>\n",
    "    <br>\n",
    "    <a href=\"https://colab.research.google.com/github/SECQUOIA/PU_CHE597_S2025/blob/main/5-Pandas_Example/Pandas_example.ipynb\" target=\"_parent\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "    </a>\n",
    "    <a href=\"https://secquoia.github.io/\">\n",
    "        <img src=\"https://img.shields.io/badge/🌲⚛️🌐-SECQUOIA-blue\" alt=\"SECQUOIA\"/>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Im4SD2x5RfIe",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Pandas---Data-Processing-Example\" data-toc-modified-id=\"Pandas---Data-Processing-Example-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Pandas - Data Processing Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ingesting-data\" data-toc-modified-id=\"Ingesting-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Ingesting data</a></span></li><li><span><a href=\"#Looking-at-your-data\" data-toc-modified-id=\"Looking-at-your-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Looking at your data</a></span></li><li><span><a href=\"#Curating-your-data\" data-toc-modified-id=\"Curating-your-data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Curating your data</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19336,
     "status": "ok",
     "timestamp": 1581369480640,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": "11084403397391204350"
     },
     "user_tz": 300
    },
    "id": "P7Pz4NcYnoc5",
    "outputId": "fd1293c6-92b6-4cd2-e677-06af69e44f4e"
   },
   "outputs": [],
   "source": [
    "# If using this on Google colab, we need to install the packages\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUbFsAtInxrq"
   },
   "source": [
    "<b>If you are using google colab you should save this notebook and any associated textfiles to their own folder on your google drive. Then you will need to adapt the following commands so that the notebook runs from the location of that folder. This is only necessary for the brief section on reading text files into Python.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use Google Drive to save/load files, set this to True\n",
    "USE_GOOGLE_DRIVE = False\n",
    "if IN_COLAB and USE_GOOGLE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Colab command to navigate to the folder holding the homework,\n",
    "    # CHANGE FOR YOUR SPECIFIC FOLDER LOCATION IN GOOGLE DRIVE\n",
    "    # Note: if there are spaces in the path, you need to precede them with a backslash '\\'\n",
    "    %cd /content/drive/My\\ Drive/CHE597/Lectures/3-Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Akw58iFcRfIg"
   },
   "source": [
    "\n",
    "\n",
    "## Pandas - Data Processing Example \n",
    "In the current notebook I've supplied IR spectra for some organic molecules, which we'll use to demonstrate some elementary data ingestion and curation activities with Pandas. The goal of this demonstration is to reinforce what we've already learned about numpy and visualization while reinforcing how they interact with Pandas in a typical workflow.  \n",
    "\n",
    "### Ingesting data\n",
    "The first thing is to take a look at the files that I have given you and to determine whether you will need to write your own parser or if the data format is compatible with one of the built in parsers from numpy or pandas. In this case, the files are commas separated with a header in the first line, which can be handled with the `pd.read_csv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1581357316145,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": "11084403397391204350"
     },
     "user_tz": 300
    },
    "id": "jF0WXELoRfIh",
    "outputId": "b11d3335-a8a4-4306-aba9-495629064215"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in data files to pandas\n",
    "data = {}\n",
    "for i in [\"1-butanol.csv\",\"diethyl_ether.csv\",\"ethylene.csv\",\"furan.csv\"]:\n",
    "    data[i] = pd.read_csv(i,header=0)\n",
    "for i in data.keys():\n",
    "    print(\"{}:\\n{}\\n\".format(i,data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at your data\n",
    "The first rule of data analysis is to look at your data. There are a host of common ailments that will jump out at you if you just look at a plot of your data. I can't emphasize how often people spend hours struggling with bad results only to eventually plot the data that they are working with and see that they have outliers, gaps, normalization issues, units problems, inconsistencies across series, etc that they recognize right away in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in data.keys(): # Loop over the distinct keys in the dictionary \n",
    "    plt.figure() # Initialize matplotlib figure\n",
    "    plt.plot(data[i][\"cm^-1\"],data[i][\"Intensity\"]) # Make the current plot\n",
    "    plt.title(i) # Add title based on the key (i.e., the filename where the data comes from)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBZ6qu_ERfIl"
   },
   "source": [
    "As domain experts, we can see that most of our spectra are report a transmittance, while `1-butanol.csv` contains an absorbance. Let's convert `1-butanol.csv` to a transmittance to match the other spectra (<b>Note:</b> the conversion is based on $T = 10^{-A}$ where we also normalize the absorbance to control for thickness):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['1-butanol.csv'][\"Intensity\"] = 10.0**(-(data['1-butanol.csv'][\"Intensity\"]/data['1-butanol.csv'][\"Intensity\"].max()))\n",
    "data['1-butanol.csv'] # Display the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the built-in pandas display in the previous cell by simply listing the pointer to the dataframe on its own line. These values are looking more like a transmittance. As usual, let's plot it to confirm that it looks sensible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check result via plot\n",
    "for i in data.keys(): # Loop over the distinct keys in the dictionary \n",
    "    plt.figure() # Initialize matplotlib figure\n",
    "    plt.plot(data[i][\"cm^-1\"],data[i][\"Intensity\"]) # Make the current plot\n",
    "    plt.title(i) # Add title based on the key (i.e., the filename where the data comes from)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of the spectra are in absorbance, what are some other observable differences? Take a second and think about this for yourself. \n",
    "\n",
    "Besides the obvious thing that the spectra have different peaks because they correspond to different compounds, I can see two things about the data that might be relevant to some analysis or machine learning applications. First, some of the spectra appear \"noisier\" than others. I put \"noisier\" here in quotations because the spectra don't necessarily have distinct measurement uncertainty (they might), but the appearance of noise can also come from the fact that the spectra are reported with different increments of wavenumber. For example:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of values in each dataframe\n",
    "for i in data.keys():\n",
    "    print(\"{}: {}\".format(i,len(data[i][\"cm^-1\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see quite clearly that spectra are not recorded at a consistent spacing in wavenumbers. The second thing that I see is that the spectra have distinct x-ranges, with some recording to higher wavenumbers than others. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the wavenumber range in each dataframe\n",
    "for i in data.keys():\n",
    "    print(\"{}: {}-{} cm^-1\".format(i,data[i][\"cm^-1\"].min(),data[i][\"cm^-1\"].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curating your data\n",
    "This kind of data misalignment is very common, and isn't necessarily a problem. However, sometimes we need our data aligned and with a fixed length (e.g., many ML models have these kinds of constraints on input). As part of our example, let's process our data so that all of the spectra are fixed length and at regular wavenumber intervals. To do this we will use one of scipy's interpolation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline # Import the interpolation function from scipy\n",
    "new_x = np.arange(350.,4001.,20) # regularly spaced x-values for our interpolated data.\n",
    "for i in data.keys(): # loop over the file keys\n",
    "    spline = CubicSpline(data[i][\"cm^-1\"],data[i][\"Intensity\"]) # Check docs for arguments and use, basic is giving it the x and y data\n",
    "    plt.figure() # Initialize matplotlib figure\n",
    "    plt.plot(new_x,spline(new_x),label=\"spline\") # Plot the spline interpolated data. We get the y-values by passing the desired x values to the spline object.\n",
    "    plt.plot(data[i][\"cm^-1\"],data[i][\"Intensity\"],label=\"original\") # Plot the original data. Note we put this second, because it makes the agreement more obvious. \n",
    "    plt.title(i) # Add title based on the filename\n",
    "    plt.legend() # Include legend to indicate the spline vs original data\n",
    "plt.show() # Show the plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the cubic spline seems to be doing a fair job interpolating the values to the specified wavenumbers, however there is a problem at the ends of the spectra where we've extrapolated rather than interpolated. This is a very common problem with splines. A simple patch is to only use the spline for interpolation, and where the wavenumber range extends beyond the data to use a null value. For example, the implementation below adds a couple lines to handle specifying `np.nan` values where there is no data in the original spectra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = np.arange(350.,4001.,50)\n",
    "for i in data.keys():\n",
    "    spline = CubicSpline(data[i][\"cm^-1\"],data[i][\"Intensity\"])\n",
    "    new_y = spline(new_x) # Get the new y-values. Save to their own array since we will replace some of them. \n",
    "    new_y[np.where((new_x<data[i][\"cm^-1\"].min()) | (new_x>data[i][\"cm^-1\"].max()))] = np.nan # Replace extrapolated elements with np.nan based on the np.where call.\n",
    "    plt.figure() \n",
    "    plt.plot(new_x,new_y,label=\"spline\")\n",
    "    plt.plot(data[i][\"cm^-1\"],data[i][\"Intensity\"],label=\"original\")\n",
    "    plt.title(i)\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> in the above cell I have used `(condition A) | (condition B)` in the `np.where` call. Here, the pipe symbol, `|`, means \"or\". If you wanted to use \"and\" to specify multiple conditions, then you would use `&`. \n",
    "\n",
    "This is looking much more reasonable. We can see some loss in fidelity near some of the peaks, but whether this is a problem depends on the application and is beyond the scope of the current example. There are also other things that we might do during preprocessing of the data, like normalize the y-range, or normalize the area depending on the application.\n",
    "\n",
    "Now that our preprocessing loop is working, let's combine the interpolated data into a shared dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = np.arange(350.,4001.,20)\n",
    "d_new = {} # Initialize a dictionary to hold the interpolated data\n",
    "for i in data.keys():\n",
    "    spline = CubicSpline(data[i][\"cm^-1\"],data[i][\"Intensity\"])\n",
    "    new_y = spline(new_x)\n",
    "    new_y[np.where((new_x<data[i][\"cm^-1\"].min()) | (new_x>data[i][\"cm^-1\"].max()))] = np.nan\n",
    "    d_new[i.split('.')[0]] = new_y # Add the values to the dictionary\n",
    "df = pd.DataFrame(d_new,index=new_x) # Use the dictionary generated by the loop to initialize the dataframe\n",
    "df # display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all of the spectra in a shared dataframe, where the wavenumber serves as the index, and the molecular name serves as the column labels. A dataframe like this is often the product of a pre-processing workflow. For example, you might add a few more lines to the code developed in the previous box to automatically ingest data from many files, apply some normalizations, interpolate to specific values, and then combine them into a dataframe for subsequent use.\n",
    "\n",
    "To check that everything is working, it is useful to plot the data in the dataframe one more time. This also reinforces how matplotlib/pandas/numpy/scipy etc are designed to work with each other, as dataframes have built-in matplotlib visualization methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot() # Note how the default behavior interprets columns and labels."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CHE597",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
